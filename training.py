import pandas as pd
from math import sqrt;
from sklearn import preprocessing
from sklearn.ensemble import RandomForestClassifier
# from sklearn.linear_model import LogisticRegression;
from sklearn.metrics import accuracy_score, r2_score, confusion_matrix, mean_absolute_error, mean_squared_error, f1_score, log_loss
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#from sklearn.externals import joblib
#import joblib as standalone lib
import joblib

#Importing Datasets
#malicious_dataset = pd.read_csv('insert malicious file name.csv')
#benign_dataset = pd.read_csv('insert benign file name.csv')

#####path file name
malicious_filename = 'malicious_flows.csv' #replace this
benign_filename = 'sample_benign_flows.csv' #replace this
validation_filename = benign_filename #replace this, it not really matter btw
##################



malicious_dataset = pd.read_csv(malicious_filename)
benign_dataset = pd.read_csv(benign_filename)

# Removing duplicated rows from benign_dataset (5380 rows removed)
benign_dataset = benign_dataset[benign_dataset.duplicated(keep=False) == False]

print(benign_dataset.duplicated(keep=False).value_counts())
print(malicious_dataset.duplicated(keep=False).value_counts())

# Combining both datasets together
all_flows = pd.concat([malicious_dataset, benign_dataset])

#Inspecting datasets for columns and rows with missing values
missing_values = all_flows.isnull().sum()
overall_percentage = (missing_values/all_flows.isnull().count())
print(f"percentage: {overall_percentage}")

# Reducing the size of the dataset to reduce the amount of time taken in training models
reduced_dataset = all_flows.sample(frac = 0.1)


#fill NaN
reduced_dataset = reduced_dataset.fillna(0)



# validation_dataset = pd.read_csv(validation_filename)
validation_dataset = all_flows.drop(reduced_dataset.index)
validation_dataset = validation_dataset[validation_dataset.duplicated(keep=False) == False]

# validateion_dataset = validation_dataset.sample(frac = 0.1)

# Examining the distribution of Malicious and Benign flows in the reduced dataset
print(f"Total malware count: \n{reduced_dataset['isMalware'].value_counts()}")

# Isolating independent and dependent variables for training dataset
#the model will try to predict reduce_y data set
#the model will be given reduce_x for learning pattern

reduced_y = reduced_dataset['isMalware']
reduced_x = reduced_dataset.drop(['isMalware'], axis=1)

# Isolating independent and dependent variables for validation dataset
#validation_y is using for scoring the model
#validation_x for testing the model
validation_y = validation_dataset['isMalware']
validation_x = validation_dataset.drop(['isMalware'], axis=1)

# Splitting datasets into training and test data
x_train, x_test, y_train, y_test = train_test_split(reduced_x, reduced_y, test_size=0.2, random_state=42)

# Training random forest classifier
randomforest_model = RandomForestClassifier(max_depth=100)
randomforest_model.fit(x_train, y_train)
rf_prediction = randomforest_model.predict(x_test)
conf_m = confusion_matrix(y_test, rf_prediction)
print(conf_m)
print('Random Forest Classifier Accuracy score: ', accuracy_score(y_test, rf_prediction))


# exit(0)
# print(validation_x.columns)
# print(x_train.columns)
validation_x = validation_x.reindex(columns=x_train.columns)
validation_x = validation_x.fillna(0)

rf_validation_prediction = randomforest_model.predict(validation_x)




print('Random Forest Classifier with validation set Accuracy score: ', accuracy_score(rf_validation_prediction, validation_y))

joblib.dump(randomforest_model, "RandomForestClassifier.joblib")

exit(0)
