import pandas as pd
from math import sqrt;
from sklearn import preprocessing
from sklearn.ensemble import RandomForestClassifier
# from sklearn.linear_model import LogisticRegression;
from sklearn.metrics import accuracy_score, r2_score, confusion_matrix,  recall_score, precision_score
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#from sklearn.externals import joblib
#import joblib as standalone lib
import joblib

#Importing Datasets
#malicious_dataset = pd.read_csv('insert malicious file name.csv')
#benign_dataset = pd.read_csv('insert benign file name.csv')

#####path file name
malicious_filename = 'malicious_flows.csv' #replace this
benign_filename = 'sample_benign_flows.csv' #replace this
validation_filename = benign_filename #replace this, it not really matter btw
##################



malicious_dataset = pd.read_csv(malicious_filename)
benign_dataset = pd.read_csv(benign_filename)

# Removing duplicated rows from benign_dataset (5380 rows removed)
benign_dataset = benign_dataset[benign_dataset.duplicated(keep=False) == False]

# print(benign_dataset.duplicated(keep=False).value_counts())
# print(malicious_dataset.duplicated(keep=False).value_counts())

# Combining both datasets together
all_flows = pd.concat([malicious_dataset, benign_dataset])
all_flows.fillna(0)
all_flows = all_flows.drop_duplicates()
# print(all_flows)
#Inspecting datasets for columns and rows with missing values
missing_values = all_flows.isnull().sum()
overall_percentage = (missing_values/all_flows.isnull().count())
print(f"percentage: {overall_percentage}")


print(f"Total rows: {all_flows.shape[0]}")
# Reducing the size of the dataset to reduce the amount of time taken in training models
reduced_dataset = all_flows.sample(frac = 1)


#fill NaN
reduced_dataset = reduced_dataset.fillna(0)



# Examining the distribution of Malicious and Benign flows in the reduced dataset
print(f"Total malware count: \n{reduced_dataset['isMalware'].value_counts()}")

# Isolating independent and dependent variables for training dataset
#the model will try to predict reduce_y data set
#the model will be given reduce_x for learning pattern
# reduced_dataset.columns = reduced_dataset.columns.astype(str)
reduced_y = reduced_dataset['isMalware']
reduced_x = reduced_dataset.drop(['isMalware'], axis=1)

# Isolating independent and dependent variables for validation dataset

# Training random forest classifier
randomforest_model = RandomForestClassifier()

setlength = 3900
index =0
rec_score = 0 
prec_score = 0
for i in range(10):
    print(f"{i} Fold")
    start_idx = index 
    end_idx = index + setlength 
    x_val = reduced_x[start_idx : end_idx]
    y_val = reduced_y[start_idx : end_idx]
    x_train = pd.concat([reduced_x[ : start_idx], reduced_x[end_idx : ]])
    y_train = pd.concat([reduced_y[ : start_idx], reduced_y[end_idx : ]])
    # print(x_train)


    randomforest_model.fit(x_train, y_train)
    y_pred = randomforest_model.predict(x_val)

    rec_score += recall_score(y_val, y_pred, average="macro")
    prec_score += precision_score(y_val, y_pred, average="macro")

    index += setlength

average_rec_score = rec_score / 10
average_prec_score = prec_score / 10

print('Random Forest Classifier Recall Score with 10 Fold Cross Validation: ', average_rec_score)
print('Random Forest Classifier Precision Score with 10 Fold Cross Validation: ', average_prec_score)
joblib.dump(randomforest_model, "RandomForestClassifier_CV.joblib")

exit(0)
